{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from timm.models.vision_transformer import Block\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "import time\n",
    "import timm.optim.optim_factory as optim_factory\n",
    "import datetime\n",
    "import copy\n",
    "from scipy.interpolate import interp1d\n",
    "from torch.utils.data import Dataset\n",
    "from torch import inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path =\"D:/study/dl/DreamDiffusion/datasets/imageNet_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['dataset', 'labels', 'images'])\n"
     ]
    }
   ],
   "source": [
    "# Load EEG from .pth files and corresponding Images\n",
    "process_file_pth = \"D:/study/dl/DreamDiffusion/datasets/eeg_5_95_std.pth\"\n",
    "try:\n",
    "    loaded_data_new = torch.load(process_file_pth)\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file {process_file_pth} does not exist.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "dataset_new = loaded_data_new.keys()\n",
    "print(dataset_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_new = loaded_data_new['dataset']\n",
    "labels_new = loaded_data_new['labels']\n",
    "images_new = loaded_data_new['images']\n",
    "\n",
    "dataset_new = np.array(dataset_new)\n",
    "labels_new = np.array(labels_new)\n",
    "images_new = np.array(images_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_image = [dataset_new[i]['image'] for i in range(len(dataset_new))]\n",
    "dataset_label = [dataset_new[i]['label'] for i in range(len(dataset_new))]\n",
    "dataset_eeg = [dataset_new[i]['eeg'] for i in range(len(dataset_new))]\n",
    "dataset_subject = [dataset_new[i]['subject'] for i in range(len(dataset_new))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_corresponding_image = []\n",
    "for index,value in enumerate(dataset_image):\n",
    "    dataset_corresponding_image.append(images_new[value])\n",
    "dataset_corresponding_image = np.array(dataset_corresponding_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "corresponding_image_directory = [main_path + img.split(\"_\")[0] + \"/\" + img for img in dataset_corresponding_image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_eeg_491= [np.array(dataset_eeg[i][:,:491]) for i in range(len(dataset_eeg))]\n",
    "dataset_eeg_491 = np.array(dataset_eeg_491)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrain_data_MAE = np.load(\"/home/utham/DreamDiffusion/Data/Recreate_DD/AlexMI(640,17,1536).npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that the frequency range of the data in pretrain_data_MAE is between 5 and 95 Hz\n",
    "# make sure that the data is standardized\n",
    "\n",
    "# Preprocessing EEG Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # take only the first 16 channels of this eeg data to get a final shape of (640, 16, 1536)\n",
    "# pretrain_data_MAE = pretrain_data_MAE[:,:16,:]\n",
    "# # repeat the channles to get shape of (640,128,1536)\n",
    "# pretrain_data_MAE = np.repeat(pretrain_data_MAE, repeats=8, axis=1)\n",
    "# # Reshape the eeg to get only 491 time points\n",
    "# pretrain_data_MAE_final = []\n",
    "# pretrain_data_MAE_final.extend(pretrain_data_MAE[:,:,:100])\n",
    "# pretrain_data_MAE_final.extend(pretrain_data_MAE[:,:,100:200])\n",
    "# pretrain_data_MAE_final.extend(pretrain_data_MAE[:,:,200:300])\n",
    "# pretrain_data_MAE_final.extend(pretrain_data_MAE[:,:,300:400])\n",
    "# pretrain_data_MAE_final.extend(pretrain_data_MAE[:,:,400:500])\n",
    "# pretrain_data_MAE_final.extend(pretrain_data_MAE[:,:,500:600])\n",
    "# pretrain_data_MAE_final.extend(pretrain_data_MAE[:,:,600:700])\n",
    "# pretrain_data_MAE_final.extend(pretrain_data_MAE[:,:,700:800])\n",
    "# pretrain_data_MAE_final.extend(pretrain_data_MAE[:,:,800:900])\n",
    "# pretrain_data_MAE_final.extend(pretrain_data_MAE[:,:,900:1000])\n",
    "# pretrain_data_MAE_final.extend(pretrain_data_MAE[:,:,1000:1100])\n",
    "# pretrain_data_MAE_final.extend(pretrain_data_MAE[:,:,1100:1200])\n",
    "# pretrain_data_MAE_final.extend(pretrain_data_MAE[:,:,1300:1400])\n",
    "# pretrain_data_MAE_final = np.array(pretrain_data_MAE_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrain_data_MAE_final = np.concatenate((pretrain_data_MAE_final, dataset_eeg_491), axis=0)\n",
    "# pretrain_data_MAE_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del pretrain_data_MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **MAIN FILES TO USE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dataset_eeg\n",
    "<br>\n",
    "<br>\n",
    "* corresponding_image_directory\n",
    "<br>\n",
    "<br>\n",
    "* dataset_subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PIPELINE 0 TO FOLLOW**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Pretraining MAE with EEG Signals from MOABB:**\n",
    "<br>\n",
    "<br>\n",
    "* **Fine Tuning Stable Diffusuion for EEG Image pairs:**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1d_sincos_pos_embed(embed_dim, length, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_l = np.arange(length, dtype=np.float32)\n",
    "\n",
    "    grid_l = grid_l.reshape([1, length])\n",
    "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid_l)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=float)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, config):\n",
    "    \"\"\"Decay the learning rate with half-cycle cosine after warmup\"\"\"\n",
    "    if epoch < config.warmup_epochs:\n",
    "        lr = config.lr * epoch / config.warmup_epochs \n",
    "    else:\n",
    "        lr = config.min_lr + (config.lr - config.min_lr) * 0.5 * \\\n",
    "            (1. + math.cos(math.pi * (epoch - config.warmup_epochs) / (config.num_epoch - config.warmup_epochs)))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if \"lr_scale\" in param_group:\n",
    "            param_group[\"lr\"] = lr * param_group[\"lr_scale\"]\n",
    "        else:\n",
    "            param_group[\"lr\"] = lr\n",
    "    return lr\n",
    "\n",
    "class PatchEmbed1D(nn.Module):\n",
    "    \n",
    "    def __init__(self, time_len=224, patch_size=1, in_chans=128, embed_dim=256):\n",
    "        super().__init__()\n",
    "        num_patches = time_len // patch_size\n",
    "        self.patch_shape = patch_size\n",
    "        self.time_len = time_len\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv1d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        B, C, V = x.shape # batch, channel, voxels\n",
    "        x = self.proj(x).transpose(1, 2).contiguous()\n",
    "        return x\n",
    "\n",
    "class MAEforEEG(nn.Module):\n",
    "    \"\"\" Masked Autoencoder with VisionTransformer backbone\n",
    "    \"\"\"\n",
    "    def __init__(self, time_len=512, patch_size=4, embed_dim=1024, in_chans=128,\n",
    "                 depth=24, num_heads=16, decoder_embed_dim=512, \n",
    "                 decoder_depth=8, decoder_num_heads=16,\n",
    "                 mlp_ratio=4., norm_layer=nn.LayerNorm, focus_range=None, focus_rate=None, img_recon_weight=1.0, \n",
    "                 use_nature_img_loss=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE encoder specifics\n",
    "        self.patch_embed = PatchEmbed1D(time_len, patch_size, in_chans, embed_dim)\n",
    "\n",
    "        num_patches = int(time_len / patch_size)\n",
    "\n",
    "        self.num_patches = num_patches\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE decoder specifics\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, in_chans * patch_size, bias=True) # encoder to decoder\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # nature image decoder specifics\n",
    "        if use_nature_img_loss:\n",
    "            self.nature_img_decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "            self.nature_img_mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "\n",
    "            self.nature_img_decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "            self.nature_img_decoder_blocks = nn.ModuleList([\n",
    "                Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "                for i in range(2)])\n",
    "\n",
    "            self.nature_img_decoder_norm = norm_layer(decoder_embed_dim)\n",
    "            self.nature_img_decoder_pred = nn.Sequential(\n",
    "                nn.Conv1d(num_patches, 512, kernel_size=1, stride=1, bias=True),\n",
    "                nn.Linear(decoder_embed_dim, 28*28, bias=True)\n",
    "            )\n",
    "            # --------------------------------------------------------------------------\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.focus_range = focus_range\n",
    "        self.focus_rate = focus_rate\n",
    "        self.img_recon_weight = img_recon_weight\n",
    "        self.use_nature_img_loss = use_nature_img_loss\n",
    "   \n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialization\n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "        pos_embed = get_1d_sincos_pos_embed(self.pos_embed.shape[-1], self.num_patches, cls_token=True)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        decoder_pos_embed = get_1d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], self.num_patches, cls_token=True)\n",
    "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        if self.use_nature_img_loss:\n",
    "            nature_img_decoder_pos_embed = get_1d_sincos_pos_embed(self.nature_img_decoder_pos_embed.shape[-1], self.num_patches, cls_token=True)\n",
    "            self.nature_img_decoder_pos_embed.data.copy_(torch.from_numpy(nature_img_decoder_pos_embed).float().unsqueeze(0))\n",
    "            torch.nn.init.normal_(self.nature_img_mask_token, std=.02)\n",
    "\n",
    "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "        w = self.patch_embed.proj.weight.data\n",
    "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv1d):\n",
    "            torch.nn.init.normal_(m.weight, std=.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 1, num_voxels)\n",
    "        imgs: [N, chan, T]\n",
    "        x: (N, L, patch_size)\n",
    "        x: [N, chan * 4, T/4]\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size\n",
    "        assert imgs.ndim == 3 and imgs.shape[1] % p == 0\n",
    "\n",
    "        # h = imgs.shape[2] // p\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], imgs.shape[1] // p, -1))\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size)\n",
    "        imgs: (N, 1, num_voxels)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size\n",
    "        h = x.shape[1]\n",
    "        \n",
    "        imgs = x.reshape(shape=(x.shape[0], -1, x.shape[2] // p))\n",
    "        return imgs.transpose(1,2)\n",
    "\n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [N, L, D], sequence\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "\n",
    "        if self.focus_range is not None:\n",
    "            len_mask = L - len_keep\n",
    "            weights = [1-self.focus_rate] * L\n",
    "            weights[self.focus_range[0] // self.patch_size : self.focus_range[1] // self.patch_size\n",
    "                        ] = [self.focus_rate] * (self.focus_range[1] // self.patch_size - self.focus_range[0] // self.patch_size)\n",
    "            weights = torch.tensor(weights).repeat(N, 1).to(x.device)\n",
    "            ids_mask = torch.multinomial(weights, len_mask, replacement=False)\n",
    "            \n",
    "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "        if self.focus_range is not None:\n",
    "            for i in range(N):\n",
    "                noise[i, ids_mask[i,:]] = 1.1  # set mask portion to 1.1 \n",
    "\n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "\n",
    "    def forward_encoder(self, x, mask_ratio):\n",
    "        # embed patches\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "        # print('encoder embed')\n",
    "        # print(x.shape)\n",
    "        # masking: length -> length * mask_ratio\n",
    "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "\n",
    "        # append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, mask, ids_restore\n",
    "\n",
    "    def forward_decoder(self, x, ids_restore = None):\n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "        # print('decoder embed')\n",
    "        # print(x.shape)\n",
    "        # append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        # x_ = torch.cat([x, mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "        # x = x_\n",
    "        # add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "        # x = x + self.decoder_pos_embed[:, 1:, :]\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "        # print(x.shape)\n",
    "        # predictor projection\n",
    "        x = self.decoder_pred(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_nature_img_decoder(self, x, ids_restore):\n",
    "        # embed tokens\n",
    "        x = self.nature_img_decoder_embed(x)\n",
    "\n",
    "        # append mask tokens to sequence\n",
    "        mask_tokens = self.nature_img_mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.nature_img_decoder_pos_embed\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.nature_img_decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.nature_img_decoder_norm(x)\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "        # predictor projection\n",
    "        # x = x.mean(dim=1, keepdim=True)\n",
    "        x = self.nature_img_decoder_pred(x)\n",
    "        x = x.view(x.shape[0], 512, 28, 28)\n",
    "\n",
    "        return x # n, 512, 28, 28\n",
    "        \n",
    "    def forward_nature_img_loss(self, inputs, reconstructions):\n",
    "        loss = ((torch.tanh(inputs) - torch.tanh(reconstructions))**2).mean()\n",
    "        if torch.isnan(reconstructions).sum():\n",
    "            print('nan in reconstructions')\n",
    "        if torch.isnan(inputs).sum():\n",
    "            print('nan in inputs')\n",
    "    \n",
    "        return loss   \n",
    "\n",
    "    def forward_loss(self, imgs, pred, mask):\n",
    "        \"\"\"\n",
    "        imgs: [N, 1, num_voxels]\n",
    "        imgs: [N, chan, T]\n",
    "        pred: [N, L, p]\n",
    "        mask: [N, L], 0 is keep, 1 is remove, \n",
    "        \"\"\"\n",
    "        imgs = imgs.transpose(1,2)\n",
    "        target = self.patchify(imgs)\n",
    "        # target = imgs.transpose(1,2)\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
    "        # loss = loss.mean()\n",
    "        loss = (loss * mask).sum() / mask.sum()  if mask.sum() != 0 else (loss * mask).sum() # mean loss on removed patches\n",
    "        return loss\n",
    "\n",
    "    def forward(self, imgs, img_features=None, valid_idx=None, mask_ratio=0.75):\n",
    "        # latent = self.forward_encoder(imgs, mask_ratio)\n",
    "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
    "            # print(x)\n",
    "        # print(latent.shape)\n",
    "        # # print(mask)\n",
    "        # print(mask.shape)\n",
    "        # # print(ids_restore)\n",
    "        # print(ids_restore.shape)\n",
    "        \n",
    "        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p]\n",
    "        # pred = self.forward_decoder(latent)  # [N, L, p]\n",
    "        # pred = pred\n",
    "        # print(pred.shape)\n",
    "        # mask=None\n",
    "        loss = self.forward_loss(imgs, pred, mask)\n",
    "        # print(self.unpatchify(pred.transpose(1,2)).shape)\n",
    "\n",
    "        if self.use_nature_img_loss and img_features is not None:\n",
    "            # valid_idx = torch.nonzero(nature_image.sum(dim=(1,2,3)) != 0).squeeze(1)\n",
    "            if len(valid_idx) != 0:\n",
    "                nature_image_recon = self.forward_nature_img_decoder(latent[valid_idx], ids_restore[valid_idx])\n",
    "                loss_nature_image_recon = self.forward_nature_img_loss(img_features, nature_image_recon)\n",
    "                if torch.isnan(loss_nature_image_recon).sum():\n",
    "                    print(loss_nature_image_recon)\n",
    "                    print(\"loss_nature_image_recon is nan\")\n",
    "                    \n",
    "                loss = loss + self.img_recon_weight*loss_nature_image_recon\n",
    "\n",
    "        return loss, pred, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NativeScalerWithGradNormCount:\n",
    "    state_dict_key = \"amp_scaler\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def __call__(self, loss, optimizer, clip_grad=None, parameters=None, create_graph=False, update_grad=True):\n",
    "        self._scaler.scale(loss).backward(create_graph=create_graph)\n",
    "        if update_grad:\n",
    "            if clip_grad is not None:\n",
    "                assert parameters is not None\n",
    "                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n",
    "                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n",
    "            else:\n",
    "                self._scaler.unscale_(optimizer)\n",
    "                norm = get_grad_norm_(parameters)\n",
    "            self._scaler.step(optimizer)\n",
    "            self._scaler.update()\n",
    "        else:\n",
    "            norm = None\n",
    "        return norm\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self._scaler.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self._scaler.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "def get_grad_norm_(parameters, norm_type: float = 2.0):\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = [p for p in parameters if p.grad is not None]\n",
    "    norm_type = float(norm_type)\n",
    "    if len(parameters) == 0:\n",
    "        return torch.tensor(0.)\n",
    "    device = parameters[0].grad.device\n",
    "    if norm_type == inf:\n",
    "        total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)\n",
    "    else:\n",
    "        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)\n",
    "    return total_norm\n",
    "\n",
    "\n",
    "def train_one_epoch(model, data_loader, optimizer, device, epoch, \n",
    "                        loss_scaler, log_writer=True, config=None, start_time=None, model_without_ddp=None, \n",
    "                        img_feature_extractor=None, preprocess=None):\n",
    "    model.train(True)\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = []\n",
    "    total_cor = []\n",
    "    accum_iter = config.accum_iter\n",
    "    for data_iter_step, (data_dcit) in enumerate(data_loader):\n",
    "        #print(\"data_iter_step:\", data_iter_step)\n",
    "        # we use a per iteration (instead of per epoch) lr scheduler\n",
    "        # print(data_iter_step)\n",
    "        # print(len(data_loader))\n",
    "        \n",
    "        if data_iter_step % accum_iter == 0:\n",
    "            adjust_learning_rate(optimizer, data_iter_step / len(data_loader) + epoch, config)\n",
    "        samples = data_dcit['eeg']\n",
    "        #print(\"Samples: \", data_dcit.shape)\n",
    "        \n",
    "        \n",
    "        valid_idx = None\n",
    "        img_features = None\n",
    "        # if img_feature_extractor is not None:\n",
    "        #     images = data_dcit['image']\n",
    "        #     valid_idx = torch.nonzero(images.sum(dim=(1,2,3)) != 0).squeeze(1)\n",
    "        #     img_feature_extractor.eval()\n",
    "        #     with torch.no_grad():\n",
    "        #         img_features = img_feature_extractor(preprocess(images[valid_idx]).to(device))['layer2']\n",
    "        samples = samples.to(device)\n",
    "        # img_features = img_features.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            loss, pred, _ = model(samples, img_features, valid_idx=valid_idx, mask_ratio=config.mask_ratio)\n",
    "            \n",
    "        #loss.backward()\n",
    "        #norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.clip_grad)\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training at step {data_iter_step} epoch {epoch}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        # loss /= accum_iter\n",
    "        loss_scaler(loss, optimizer, parameters=model.parameters(), clip_grad=config.clip_grad)\n",
    "\n",
    "        # if (data_iter_step + 1) % accum_iter == 0:\n",
    "        # cal the cor\n",
    "        pred = pred.to('cpu').detach()\n",
    "        samples = samples.to('cpu').detach()\n",
    "        # pred = pred.transpose(1,2) #model_without_ddp.unpatchify(pred)\n",
    "        pred = model_without_ddp.unpatchify(pred)\n",
    "        # print(pred.shape)\n",
    "        # print(samples.shape)\n",
    "        # for p, s in zip(pred, samples):\n",
    "        #     print(p[0], s[0])\n",
    "        #     print(torch.cat([p[0].unsqueeze(0), s[0].unsqueeze(0)],axis=0))\n",
    "        #     print(torch.corrcoef(torch.cat([p[0].unsqueeze(0), s[0].unsqueeze(0)],axis=0)))\n",
    "        #     print(torch.corrcoef(torch.cat([p[0].unsqueeze(0), s[0].unsqueeze(0)],axis=0))[0,1])\n",
    "            \n",
    "        cor = torch.mean(torch.tensor([torch.corrcoef(torch.cat([p[0].unsqueeze(0), s[0].unsqueeze(0)],axis=0))[0,1] for p, s in zip(pred, samples)])).item()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss.append(loss_value)\n",
    "        total_cor.append(cor)\n",
    "        # if device == torch.device('cuda:0'):\n",
    "        #     lr = optimizer.param_groups[0][\"lr\"]\n",
    "        #     print('train_loss_step:', np.mean(total_loss), 'lr:', lr, 'cor', np.mean(total_cor))\n",
    "\n",
    "    # if True:\n",
    "    #     lr = optimizer.param_groups[0][\"lr\"]\n",
    "    #     print('train_loss_step', np.mean(total_loss))\n",
    "    #     print('lr', lr)\n",
    "    #     print('cor', np.mean(total_cor))\n",
    "    #     if start_time is not None:\n",
    "    #         print('time (min)', (time.time() - start_time)/60.0)\n",
    "    if config.local_rank:        \n",
    "        print(f'[Epoch {epoch}] loss: {np.mean(total_loss)}')\n",
    "\n",
    "    return np.mean(total_cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config_MBM_EEG:\n",
    "    # configs for fmri_pretrain.py\n",
    "    def __init__(self):\n",
    "    # --------------------------------------------\n",
    "    # MAE for fMRI\n",
    "        # Training Parameters\n",
    "        self.lr = 2.5e-3\n",
    "        self.min_lr = 1e-6\n",
    "        self.weight_decay = 0.01\n",
    "        self.num_epoch = 100\n",
    "        self.warmup_epochs = 2\n",
    "        self.batch_size = 32\n",
    "        self.clip_grad = 0.8\n",
    "        \n",
    "        # Model Parameters\n",
    "        self.mask_ratio = 0.1\n",
    "        self.patch_size = 4 #  1\n",
    "        self.embed_dim = 1024 #256 # has to be a multiple of num_heads\n",
    "        self.decoder_embed_dim = 512 #128\n",
    "        self.depth = 24\n",
    "        self.num_heads = 16\n",
    "        self.decoder_num_heads = 32\n",
    "        self.mlp_ratio = 1.0\n",
    "\n",
    "        # Project setting\n",
    "        self.root_path = '/home/utham/DreamDiffusion/Data/Recreate_DD/'\n",
    "        self.output_path = '/home/utham/DreamDiffusion/Data/Recreate_DD/output_mae/'\n",
    "        self.seed = 2022\n",
    "        self.roi = 'VC'\n",
    "        self.aug_times = 1\n",
    "        self.num_sub_limit = None\n",
    "        self.include_hcp = True\n",
    "        self.include_kam = True\n",
    "        self.accum_iter = 4\n",
    "\n",
    "        self.use_nature_img_loss = False\n",
    "        self.img_recon_weight = 0.5\n",
    "        self.focus_range = None # [0, 1500] # None to disable it\n",
    "        self.focus_rate = 0.6\n",
    "\n",
    "        # distributed training\n",
    "        self.local_rank = 0\n",
    "        self.gpu_ids=[3,4,5,6,7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config_MBM_EEG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eeg_pretrain_dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super(eeg_pretrain_dataset, self).__init__()\n",
    "        self.data = data\n",
    "        self.data_len = 512\n",
    "        self.data_chan = 128\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        eeg_sample = self.data[index]\n",
    "\n",
    "        if eeg_sample.shape[-1] > self.data_len:\n",
    "            idx = np.random.randint(0, int(eeg_sample.shape[-1] - self.data_len) + 1)\n",
    "            eeg_sample = eeg_sample[:, idx: idx + self.data_len]\n",
    "        else:\n",
    "            x = np.linspace(0, 1, eeg_sample.shape[-1])\n",
    "            x2 = np.linspace(0, 1, self.data_len)\n",
    "            f = interp1d(x, eeg_sample)\n",
    "            eeg_sample = f(x2)\n",
    "\n",
    "        ret = np.zeros((self.data_chan, self.data_len))\n",
    "        if self.data_chan > eeg_sample.shape[-2]:\n",
    "            for i in range((self.data_chan // eeg_sample.shape[-2])):\n",
    "                ret[i * eeg_sample.shape[-2]: (i + 1) * eeg_sample.shape[-2], :] = eeg_sample\n",
    "            if self.data_chan % eeg_sample.shape[-2] != 0:\n",
    "                ret[-(self.data_chan % eeg_sample.shape[-2]):, :] = eeg_sample[: (self.data_chan % eeg_sample.shape[-2]), :]\n",
    "        elif self.data_chan < eeg_sample.shape[-2]:\n",
    "            idx2 = np.random.randint(0, int(eeg_sample.shape[-2] - self.data_chan) + 1)\n",
    "            ret = eeg_sample[idx2: idx2 + self.data_chan, :]\n",
    "        elif self.data_chan == eeg_sample.shape[-2]:\n",
    "            ret = eeg_sample\n",
    "\n",
    "        ret = ret / 10  # reduce an order\n",
    "        ret = torch.from_numpy(ret).float()\n",
    "        return {'eeg': ret}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(config, epoch, model, optimizer, loss_scaler, checkpoint_paths):\n",
    "    #print(\"FKU\")\n",
    "    os.makedirs(checkpoint_paths, exist_ok=True)\n",
    "    to_save = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'scaler': loss_scaler.state_dict(),\n",
    "        'config': config,\n",
    "    }\n",
    "    torch.save(to_save, os.path.join(checkpoint_paths, 'checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(config, model, checkpoint_path ):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    print(f'Model loaded with {checkpoint_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, data_loader, optimizer, device, epoch, \n",
    "                        loss_scaler, log_writer=None, config=None, start_time=None, model_without_ddp=None, \n",
    "                        img_feature_extractor=None, preprocess=None):\n",
    "    model.train(True)\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = []\n",
    "    total_cor = []\n",
    "    accum_iter = config.accum_iter\n",
    "    for data_iter_step, (data_dcit) in enumerate(data_loader):\n",
    "        \n",
    "        # we use a per iteration (instead of per epoch) lr scheduler\n",
    "        # print(data_iter_step)\n",
    "        # print(len(data_loader))\n",
    "        \n",
    "        if data_iter_step % accum_iter == 0:\n",
    "            ut.adjust_learning_rate(optimizer, data_iter_step / len(data_loader) + epoch, config)\n",
    "        samples = data_dcit['eeg']\n",
    "        \n",
    "        img_features = None\n",
    "        valid_idx = None\n",
    "        if img_feature_extractor is not None:\n",
    "            images = data_dcit['image']\n",
    "            valid_idx = torch.nonzero(images.sum(dim=(1,2,3)) != 0).squeeze(1)\n",
    "            img_feature_extractor.eval()\n",
    "            with torch.no_grad():\n",
    "                img_features = img_feature_extractor(preprocess(images[valid_idx]).to(device))['layer2']\n",
    "        samples = samples.to(device)\n",
    "        # img_features = img_features.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            loss, pred, _ = model(samples, img_features, valid_idx=valid_idx, mask_ratio=config.mask_ratio)\n",
    "        # loss.backward()\n",
    "        # norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.clip_grad)\n",
    "        # optimizer.step()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training at step {data_iter_step} epoch {epoch}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        # loss /= accum_iter\n",
    "        loss_scaler(loss, optimizer, parameters=model.parameters(), clip_grad=config.clip_grad)\n",
    "\n",
    "        # if (data_iter_step + 1) % accum_iter == 0:\n",
    "        # cal the cor\n",
    "        pred = pred.to('cpu').detach()\n",
    "        samples = samples.to('cpu').detach()\n",
    "        # pred = pred.transpose(1,2) #model_without_ddp.unpatchify(pred)\n",
    "        pred = model_without_ddp.unpatchify(pred)\n",
    "        # print(pred.shape)\n",
    "        # print(samples.shape)\n",
    "        # for p, s in zip(pred, samples):\n",
    "        #     print(p[0], s[0])\n",
    "        #     print(torch.cat([p[0].unsqueeze(0), s[0].unsqueeze(0)],axis=0))\n",
    "        #     print(torch.corrcoef(torch.cat([p[0].unsqueeze(0), s[0].unsqueeze(0)],axis=0)))\n",
    "        #     print(torch.corrcoef(torch.cat([p[0].unsqueeze(0), s[0].unsqueeze(0)],axis=0))[0,1])\n",
    "            \n",
    "        cor = torch.mean(torch.tensor([torch.corrcoef(torch.cat([p[0].unsqueeze(0), s[0].unsqueeze(0)],axis=0))[0,1] for p, s in zip(pred, samples)])).item()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss.append(loss_value)\n",
    "        total_cor.append(cor)\n",
    "        if device == torch.device('cuda:0'):\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            print('train_loss_step:', np.mean(total_loss), 'lr:', lr, 'cor', np.mean(total_cor))\n",
    "\n",
    "    if log_writer is not None:\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        log_writer.log('train_loss_step', np.mean(total_loss), step=epoch)\n",
    "        log_writer.log('lr', lr, step=epoch)\n",
    "        log_writer.log('cor', np.mean(total_cor), step=epoch)\n",
    "        if start_time is not None:\n",
    "            log_writer.log('time (min)', (time.time() - start_time)/60.0, step=epoch)\n",
    "    if config.local_rank == 0:        \n",
    "        print(f'[Epoch {epoch}] loss: {np.mean(total_loss)}')\n",
    "\n",
    "    return np.mean(total_cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    # print('num of gpu:')\n",
    "    # print(torch.cuda.device_count())\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        torch.cuda.set_device(config.local_rank) \n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "    output_path = os.path.join(config.root_path, 'results', 'eeg_pretrain',  '%s'%(datetime.datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")))\n",
    "    config.output_path = output_path\n",
    "    # logger = wandb_logger(config) if config.local_rank == 0 else None\n",
    "    logger = None\n",
    "    \n",
    "    if config.local_rank == 0:\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        create_readme(config, output_path)\n",
    "    \n",
    "    device = torch.device(f'cuda:{config.local_rank}') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    torch.manual_seed(config.seed)\n",
    "    np.random.seed(config.seed)\n",
    "\n",
    "    # create dataset and dataloader\n",
    "    dataset_pretrain = eeg_pretrain_dataset(path='../dreamdiffusion/datasets/mne_data/', roi=config.roi, patch_size=config.patch_size,\n",
    "                transform=fmri_transform, aug_times=config.aug_times, num_sub_limit=config.num_sub_limit, \n",
    "                include_kam=config.include_kam, include_hcp=config.include_hcp)\n",
    "   \n",
    "    print(f'Dataset size: {len(dataset_pretrain)}\\n Time len: {dataset_pretrain.data_len}')\n",
    "    sampler = torch.utils.data.DistributedSampler(dataset_pretrain, rank=config.local_rank) if torch.cuda.device_count() > 1 else None \n",
    "\n",
    "    dataloader_eeg = DataLoader(dataset_pretrain, batch_size=config.batch_size, sampler=sampler, \n",
    "                shuffle=(sampler is None), pin_memory=True)\n",
    "\n",
    "    # create model\n",
    "    config.time_len=dataset_pretrain.data_len\n",
    "    model = MAEforEEG(time_len=dataset_pretrain.data_len, patch_size=config.patch_size, embed_dim=config.embed_dim,\n",
    "                    decoder_embed_dim=config.decoder_embed_dim, depth=config.depth, \n",
    "                    num_heads=config.num_heads, decoder_num_heads=config.decoder_num_heads, mlp_ratio=config.mlp_ratio,\n",
    "                    focus_range=config.focus_range, focus_rate=config.focus_rate, \n",
    "                    img_recon_weight=config.img_recon_weight, use_nature_img_loss=config.use_nature_img_loss)   \n",
    "    model.to(device)\n",
    "    model_without_ddp = model\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "        model = DistributedDataParallel(model, device_ids=[config.local_rank], output_device=config.local_rank, find_unused_parameters=config.use_nature_img_loss)\n",
    "\n",
    "    param_groups = optim_factory.add_weight_decay(model, config.weight_decay)\n",
    "    optimizer = torch.optim.AdamW(param_groups, lr=config.lr, betas=(0.9, 0.95))\n",
    "    print(optimizer)\n",
    "    loss_scaler = NativeScaler()\n",
    "\n",
    "    if logger is not None:\n",
    "        logger.watch_model(model,log='all', log_freq=1000)\n",
    "\n",
    "    cor_list = []\n",
    "    start_time = time.time()\n",
    "    print('Start Training the EEG MAE ... ...')\n",
    "    img_feature_extractor = None\n",
    "    preprocess = None\n",
    "    if config.use_nature_img_loss:\n",
    "        from torchvision.models import resnet50, ResNet50_Weights\n",
    "        from torchvision.models.feature_extraction import create_feature_extractor\n",
    "        weights = ResNet50_Weights.DEFAULT\n",
    "        preprocess = weights.transforms()\n",
    "        m = resnet50(weights=weights)   \n",
    "        img_feature_extractor = create_feature_extractor(m, return_nodes={f'layer2': 'layer2'}).to(device).eval()\n",
    "        for param in img_feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    for ep in range(config.num_epoch):\n",
    "        \n",
    "        if torch.cuda.device_count() > 1: \n",
    "            sampler.set_epoch(ep) # to shuffle the data at every epoch\n",
    "        cor = train_one_epoch(model, dataloader_eeg, optimizer, device, ep, loss_scaler, logger, config, start_time, model_without_ddp,\n",
    "                            img_feature_extractor, preprocess)\n",
    "        cor_list.append(cor)\n",
    "        if (ep % 20 == 0 or ep + 1 == config.num_epoch) and config.local_rank == 0: #and ep != 0\n",
    "            # save models\n",
    "        # if True:\n",
    "            save_model(config, ep, model_without_ddp, optimizer, loss_scaler, os.path.join(output_path,'checkpoints'))\n",
    "            # plot figures\n",
    "            plot_recon_figures(model, device, dataset_pretrain, output_path, 5, config, logger, model_without_ddp)\n",
    "            \n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print('Training time {}'.format(total_time_str))\n",
    "    if logger is not None:\n",
    "        logger.log('max cor', np.max(cor_list), step=config.num_epoch-1)\n",
    "        logger.finish()\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
